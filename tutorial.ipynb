{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLLM generation & curation tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup keys and experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "\n",
    "from cllm.utils import *\n",
    "from cllm.llm_gen import *\n",
    "from cllm.data_loader import *\n",
    "\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# API KEY SETUP INSTRUCTIONS\n",
    "#############################################################\n",
    "\n",
    "# for vllm\n",
    "# api_key = \"EMPTY\"\n",
    "# api_base = \"http://localhost:8000/v1\"\n",
    "\n",
    "# for together\n",
    "# api_key = \"add together api key\"\n",
    "# api_base = \"https://api.together.xyz/v1\"\n",
    "\n",
    "\n",
    "# for azure openai\n",
    "# api_key = \"EMPTY\"\n",
    "# api_base = \"add azure deployment link\"\n",
    "\n",
    "# for openai\n",
    "# api_key = \"EMPTY\"\n",
    "# api_base = DO NOT INCLUDE\n",
    "\n",
    "#############################################################\n",
    "\n",
    "api_details = {\n",
    "     \"api_base\": \"add api base\",\n",
    "     \"api_version\": \"2023-07-01-preview\",\n",
    "     \"api_key\": \"add api key\",\n",
    "}\n",
    "\n",
    "\n",
    "model_short_name = 'mixtral' # 'gpt-4' (do not use other short names)\n",
    "model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # \"gpt4_20230815\" (use name of your model deployment)\n",
    "llm_serving='together' # supported 'azure_openai', 'together', 'vllm'\n",
    "\n",
    "seed = 0\n",
    "ns = 20 # n_samples per class. e.g. if binary = 40 samples (i.e. 20 per class)\n",
    "dataset = 'compas'\n",
    "n_synthetic=10 # just to test --- normall should be 1000\n",
    "n_processes = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat, df_label, df = get_data(dataset=dataset, seed=seed)\n",
    "\n",
    "X_train, X_remain, y_train, y_remain = sample_and_split(df_feat, df_label, ns=ns, seed=seed)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_remain, y_remain, test_size=0.5, random_state=seed\n",
    ")\n",
    "\n",
    "\n",
    "X_train_orig = deepcopy(X_train)\n",
    "y_train_orig = deepcopy(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "response_schemas = []\n",
    "\n",
    "example_df = pd.concat([X_train_orig, y_train_orig], axis=1)\n",
    "\n",
    "# Shuffle\n",
    "example_df = example_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "for idx, col in enumerate(list(example_df.columns)):\n",
    "    if col == 'y':\n",
    "        resp = ResponseSchema(name='y',\n",
    "                        description=f\"binary label, {col}\", )\n",
    "    else:\n",
    "        resp = ResponseSchema(name=col,\n",
    "                        description=f\"feature column\", )\n",
    "    response_schemas.append(resp)\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "\n",
    "generator_template = \"\"\"\\\n",
    "You are a synthetic data generator. \n",
    "Your goal is to produce data which mirrors \\\n",
    "the given examples in causal structure and feature and label distributions \\\n",
    "but also produces as diverse samples as possible\n",
    "\n",
    "I will give you real examples first\n",
    "\n",
    "Leverage your knowledge about criminal recividsm to generate 1000 realistic but diverse samples. \n",
    "\n",
    "example data: {data}\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "DO NOT COPY THE EXAMPLES but generate realistic but new and diverse samples which have the correct label conditioned on the features.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=generator_template)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retries = 4  # Max retries you want to attempt\n",
    "\n",
    "while retries > 0:\n",
    "    try:\n",
    "\n",
    "        if len(example_df)>20:\n",
    "            ic_samples=20\n",
    "        else:\n",
    "            ic_samples=len(example_df)\n",
    "        \n",
    "        print(f'Running {dataset}, {seed}, {model} --- {n_processes}')\n",
    "        df_llm = llm_gen(prompt, generator_template, format_instructions, example_df, \n",
    "                        n_samples=n_synthetic,\n",
    "                        temperature=0.9,\n",
    "                        max_tokens=1000, model=model, \n",
    "                        n_processes=n_processes,\n",
    "                        ic_samples=ic_samples, \n",
    "                        llm_serving=llm_serving, \n",
    "                        api_details=api_details)\n",
    "    \n",
    "        \n",
    "        break  # if successful, break out of the loop\n",
    "    except Exception as e:\n",
    "        time.sleep(120)\n",
    "        print(f\"Error: {e}. Retrying with reduced n_processes...\")\n",
    "        n_processes = int(n_processes/2)\n",
    "        retries -= 1\n",
    "        if n_processes < 1:\n",
    "            print(\"Error: Minimum n_processes reached. Exiting...\")\n",
    "            break\n",
    "# try:\n",
    "tmp_df = df_llm.astype(example_df.dtypes)\n",
    "df_llm = tmp_df\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process LLM generated data to have the same data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = df_llm.dropna()\n",
    "df_llm = df_llm[~df_llm.apply(lambda row: any([isinstance(cell, str) and cell in ['integer', 'float', 'numeric', 'categorical', 'number', 'No', 'Yes', 'continuous', 'age in years', 'string'] for cell in row]), axis=1)]\n",
    "\n",
    "example_df = deepcopy(X_train_orig)\n",
    "example_df['y'] = deepcopy(y_train_orig)\n",
    "\n",
    "try:\n",
    "    df_llm = df_llm.astype(example_df.dtypes)\n",
    "except:\n",
    "    # Assuming the dtypes from the example_df['Dtrain'].dataframe() is what you want\n",
    "    target_dtypes = example_df.dtypes.to_dict()\n",
    "\n",
    "    problematic_rows = set()\n",
    "\n",
    "    for col, dtype in target_dtypes.items():\n",
    "        for index, value in df[col].items():\n",
    "            try:\n",
    "                _ = dtype.type(value)  # Try to convert the value\n",
    "            except Exception:\n",
    "                problematic_rows.add(index)\n",
    "\n",
    "    # Convert the problematic rows to a list and sort them\n",
    "    problematic_rows = sorted(list(problematic_rows))\n",
    "\n",
    "    # Drop the problematic rows\n",
    "    df_llm.drop(problematic_rows, inplace=True)\n",
    "\n",
    "    # Identify rows where any cell is of type list\n",
    "    rows_with_lists = df.applymap(lambda x: isinstance(x, list)).any(axis=1)\n",
    "\n",
    "    # Drop those rows\n",
    "    df_llm = df_llm[~rows_with_lists]\n",
    "\n",
    "    df_llm = df_llm.astype(example_df.dtypes)\n",
    "\n",
    "\n",
    "df_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.curation import data_centric_curation\n",
    "\n",
    "X_check = df_llm.drop(columns=['y'])\n",
    "y_check = df_llm['y'].values.astype(int)\n",
    "\n",
    "curation_metric = 'aleatoric'\n",
    "curation_ythresh=0.2\n",
    "curation_xthresh=0 #adaptive\n",
    "\n",
    "easy_train, ambig_train, unlearnable_train, Curator_xgb = data_centric_curation(X_train_orig, y_train_orig, X_check, y_check, \n",
    "                 curation_metric=curation_metric, retrain=False, nest = 100, \n",
    "                 curation_ythresh=curation_ythresh, curation_xthresh=curation_xthresh)\n",
    "\n",
    "curated_train_ids = np.concatenate((easy_train, ambig_train))\n",
    "curated_train_ids, unlearnable_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

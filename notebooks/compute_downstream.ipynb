{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from cllm.utils import *\n",
    "from cllm.curation import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "n_synthetic = 1000\n",
    "retrain = False\n",
    "nest=100\n",
    "downstream = \"xgb\"\n",
    "curation_ythresh=0.2\n",
    "curation_xthresh = 0.15\n",
    "temp = 0.9\n",
    "\n",
    "# Factors to evaluate\n",
    "seeds = [0,1,2,3,4,5,6,7,8,9]\n",
    "n_samples = [10,20,50,100] \n",
    "datasets = [\"covid\", \"adult\", \"seer\", \"cutract\", \"maggic\"]\n",
    "\n",
    "ylabel_map = {\"covid\": \"is_dead\",\n",
    "            \"adult\": \"salary\",\n",
    "            \"compas\": \"y\",\n",
    "            \"seer\": \"mortCancer\",\n",
    "            \"cutract\":\"mortCancer\",\n",
    "            \"maggic\": \"death_all\",\n",
    "            \"support\": \"death\",\n",
    "            \"bio\": \"y\", \n",
    "            \"higgs\": \"y\",\n",
    "            \"drug\": \"target\"\n",
    "            }\n",
    "\n",
    "for dataset in datasets:  \n",
    "\n",
    "    for ns in n_samples:\n",
    "        performance_all = []\n",
    "        acc_list_all = []\n",
    "        auc_list_all = []\n",
    "        f1_list_all = []\n",
    "\n",
    "        acc_clf_all = []\n",
    "        auc_clf_all = []\n",
    "        f1_clf_all = []\n",
    "\n",
    "        results_plot_all = []\n",
    "        results_data_all = []\n",
    "        subset_dict_all = []\n",
    "         \n",
    "        for seed in seeds:\n",
    "            try:\n",
    "                    print(f\"Computing for {dataset} with seed {seed} and ns {ns}\")\n",
    "\n",
    "                    dfs_dicts = {}\n",
    "                    results = {}\n",
    "                    subset_dict = {}\n",
    "\n",
    "                    ylabel =  ylabel_map[dataset]\n",
    "\n",
    "                    # Get the GPT-4 Generated data\n",
    "                    gpt_model = \"gpt-4\"\n",
    "                    tmp_df = process_gpt(dataset=dataset, n_synthetic=n_synthetic, temp=temp, gpt_model=gpt_model, ns=ns, seed=seed)\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results[\"gpt-4\"] =  {\"X\": tmp_df.drop(columns=[\"target\"]), \"y\": tmp_df[\"target\"], \"df\": tmp_df}\n",
    "\n",
    "                    # Get the GPT-3.5 Generated data\n",
    "                    gpt_model = \"gpt3\"\n",
    "                    tmp_df = process_gpt(dataset=dataset, n_synthetic=n_synthetic, temp=temp, gpt_model=gpt_model, ns=ns, seed=seed)\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results[gpt_model] =  {\"X\": tmp_df.drop(columns=[\"target\"]), \"y\": tmp_df[\"target\"], \"df\": tmp_df}\n",
    "     \n",
    "                    # Extract the Great datasets\n",
    "                    filename = f\"../save_dfs/great_pipeline_{dataset}_{seed}_{ns}.pickle\"\n",
    "                    if os.path.exists(filename):\n",
    "                        with open(filename, \"rb\") as f:\n",
    "                                    great_df = pickle.load(f)\n",
    "\n",
    "                    tmp_df = great_df[\"great\"][\"X\"]\n",
    "                    tmp_df[\"target\"] = great_df[\"great\"][\"y\"]\n",
    "                    tmp_df = tmp_df.reset_index(drop=True)\n",
    "                    results[\"great\"] =  {\"X\": tmp_df.drop(columns=[\"target\"]), \"y\": tmp_df[\"target\"], \"df\": tmp_df}\n",
    "  \n",
    "\n",
    "                    # Get all the baseline results\n",
    "                    filename = f\"../save_dfs/pipeline_{dataset}_{seed}_{ns}.pickle\"\n",
    "                    with open(filename, \"rb\") as f:\n",
    "                            df = pickle.load(f)\n",
    "\n",
    "                    # Extract each baseline method, as well as Dorig, Doracle, Dtest\n",
    "                    for model in list(df.keys()):\n",
    "\n",
    "                        if \"great\" in model:\n",
    "                            continue\n",
    "\n",
    "                        tmp_df = df[model][\"X\"]\n",
    "                        tmp_df[\"target\"] = df[model][\"y\"]\n",
    "\n",
    "                        # reset index\n",
    "                        tmp_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "                        if model == \"Original\":\n",
    "                            X_train_orig, y_train_orig = tmp_df.drop(columns=[\"target\"]), tmp_df[\"target\"]\n",
    "                        elif model == \"Oracle\":\n",
    "                            X_oracle, y_oracle = tmp_df.drop(columns=[\"target\"]), tmp_df[\"target\"]\n",
    "                        elif model == \"Test\":\n",
    "                            X_test, y_test = tmp_df.drop(columns=[\"target\"]), tmp_df[\"target\"]\n",
    "\n",
    "                        if model!=\"Test\":\n",
    "                            results[model] =  {\"X\": tmp_df.drop(columns=[\"target\"]), \"y\": tmp_df[\"target\"], \"df\": tmp_df}\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    acc_list = []\n",
    "                    auc_list = []\n",
    "                    recall_list = []\n",
    "                    precision_list = []\n",
    "                    f1_list = []\n",
    "                    model_names = []\n",
    "                    acc_per_clf = {}\n",
    "                    auc_per_clf = {}\n",
    "                    f1_per_clf = {}\n",
    "\n",
    "\n",
    "                    print(\"Applying curation and store subsets...\")\n",
    "                    results_models = list(results.keys())\n",
    "\n",
    "                    # enrich results dict\n",
    "                    for model in results_models:\n",
    "                        if model==\"Original\" or model==\"Oracle\":\n",
    "                            continue\n",
    "\n",
    "                        X_eval, y_eval = results[model][\"X\"], results[model][\"y\"]\n",
    "\n",
    "                        df_sample = deepcopy(X_eval)\n",
    "                        df_sample[\"target\"] = y_eval\n",
    "                        if len(df_sample)>1000:\n",
    "                            df_sample = df_sample.sample(n=1000, random_state=seed,).reset_index(drop=True)\n",
    "                        X_eval, y_eval = df_sample.drop(columns=[\"target\"]), df_sample[\"target\"]\n",
    "\n",
    "                        # CLLM Curation Mechanism\n",
    "                        easy_train, ambig_train, hard_train, dataiq_model  = data_centric_curation(X_train_orig = X_train_orig, \n",
    "                                    y_train_orig= y_train_orig, \n",
    "                                    X_check = X_eval,\n",
    "                                    y_check = y_eval,\n",
    "                                    retrain=retrain,\n",
    "                                    nest=nest,\n",
    "                                    curation_ythresh=curation_ythresh,\n",
    "                                    curation_xthresh=curation_xthresh,\n",
    "                                    )\n",
    "                        \n",
    "                        df_save = deepcopy(X_eval)\n",
    "                        df_save[\"target\"] = y_eval\n",
    "\n",
    "                        # Store data subsets\n",
    "                        subset_dict[model] = {\"easy\": df_save.iloc[easy_train,:], \"ambig\": df_save.iloc[ambig_train,:], \"hard\": df_save.iloc[hard_train,:], \"easy_ambig\": df_save.iloc[np.concatenate((easy_train, ambig_train)),:], \"all\": df_save, \"easy_ids\": easy_train, \"ambig_ids\": ambig_train, \"hard_ids\": hard_train, \"easy_ambig_ids\": np.concatenate((easy_train, ambig_train))}\n",
    "                        \n",
    "                        # results[f\"{model}_easy\"] = {\"X\": X_eval.iloc[easy_train,:], \"y\": y_eval[easy_train], \"df\": df_save.iloc[easy_train,:]}\n",
    "                        # results[f\"{model}_ambig\"] = {\"X\": X_eval.iloc[ambig_train,:], \"y\": y_eval[ambig_train], \"df\": df_save.iloc[ambig_train,:]}\n",
    "                        \n",
    "                        # The curated is after we remove \"Hard\" synthetic samples\n",
    "                        results[f\"{model}_curated\"] = {\"X\": X_eval.iloc[np.concatenate((easy_train, ambig_train)),:], \"y\": y_eval[np.concatenate((easy_train, ambig_train))], \"df\": df_save.iloc[np.concatenate((easy_train, ambig_train)),:]}\n",
    "\n",
    "              \n",
    "                    print(\"Fitting downstream models on the different stored datasets...\")\n",
    "                    for idx, model in enumerate(list(results.keys())):\n",
    "                        \n",
    "                        X_eval, y_eval = results[model][\"X\"], results[model][\"y\"]\n",
    "                        clf1 = XGBClassifier(n_estimators=nest, random_state=seed)\n",
    "                        clf2 = RandomForestClassifier(n_estimators=nest, random_state=seed)\n",
    "                        clf3 = LogisticRegression(random_state=seed)\n",
    "                        clf4 = DecisionTreeClassifier(random_state=seed)\n",
    "  \n",
    "                        X_eval, y_eval = results[model][\"X\"], results[model][\"y\"]\n",
    "                        try:\n",
    "                            scaler = preprocessing.StandardScaler().fit(X_eval)\n",
    "                        except:\n",
    "                             print(model)\n",
    "\n",
    "                        # Perform model evaluation\n",
    "                        acc1, rec1, prec1, f11, auc1, clf1 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf1)\n",
    "                        acc2, rec2, prec2, f12, auc2, clf2 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf2)\n",
    "                        acc3, rec3, prec3, f13, auc3, clf3 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf3)\n",
    "                        acc4, rec4, prec4, f14, auc4, clf4 = evaluate_model(scaler.transform(X_eval), y_eval, scaler.transform(X_test), y_test, clf4)\n",
    "\n",
    "                        acc_per_clf[model] = {\"xgb\": acc1, \"rf\": acc2, \"lr\": acc3, \"dt\": acc4}\n",
    "                        auc_per_clf[model] = {\"xgb\": auc1, \"rf\": auc2, \"lr\": auc3, \"dt\": auc4}\n",
    "                        f1_per_clf[model] = {\"xgb\": f11, \"rf\": f12, \"lr\": f13, \"dt\": f14}\n",
    "\n",
    "                        acc = np.mean([acc1, acc2, acc3])\n",
    "                        rec = np.mean([rec1, rec2, rec3])\n",
    "                        prec = np.mean([prec1, prec2, prec3])\n",
    "                        f1 = np.mean([f11, f12, f13])\n",
    "                        auc = np.mean([auc1, auc2, auc3])\n",
    "                        \n",
    "                        acc_list.append(acc)\n",
    "                        recall_list.append(rec)\n",
    "                        precision_list.append(prec)\n",
    "                        f1_list.append(f1)\n",
    "                        auc_list.append(auc)\n",
    "                        model_names.append(model)\n",
    "\n",
    "                        if model!=\"Original\" or model!=\"Oracle\":\n",
    "\n",
    "                            if model==\"Oracle\":\n",
    "                                oracle_idx = idx\n",
    "\n",
    "                            continue\n",
    "\n",
    "                    # Store results\n",
    "                    performance_dict = {\"Accuracy\": acc_list, \"AUC\": auc_list, \"Group\": model_names, \"Recall\": recall_list, \"Precision\": precision_list, \"F1\": f1_list, \"Dataset\": dataset, \"Seed\": seed, \"acc_clf\": acc_per_clf, \"auc_clf\": auc_per_clf, \"f1_clf\": f1_per_clf}\n",
    "                    performance_all.append(performance_dict)\n",
    "\n",
    "                    acc_list_all.append(acc_list)\n",
    "                    auc_list_all.append(auc_list)\n",
    "                    f1_list_all.append(f1_list)\n",
    "                    acc_clf_all.append(acc_per_clf)\n",
    "                    auc_clf_all.append(auc_per_clf)\n",
    "                    f1_clf_all.append(f1_per_clf)\n",
    "\n",
    "                    results_data_all.append(results)\n",
    "                    subset_dict_all.append(subset_dict)\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Error in fitting models: \", e)\n",
    "                continue    \n",
    "        \n",
    "            \n",
    "            try:\n",
    "\n",
    "                acc_list = np.mean(np.array(acc_list_all), axis=0)\n",
    "                auc_list = np.mean(np.array(auc_list_all), axis=0)\n",
    "                f1_list = np.mean(np.array(f1_list_all), axis=0)\n",
    "\n",
    "                df = pd.DataFrame({\"Accuracy\": acc_list, \"AUC\": auc_list, \"Group\": model_names})\n",
    "                df = pd.melt(df, id_vars=[\"Group\"], value_vars=[\"Accuracy\", \"AUC\"])\n",
    "                df.columns = [\"Group\", \"Metric\", \"Score\"]\n",
    "\n",
    "                results_plot_all.append(df)\n",
    "\n",
    "                # create a dict to store all the results\n",
    "                all_results = {\"model_order\": list(results.keys()),\n",
    "                                \"performance_all\": performance_all, \n",
    "                                \"acc_list_all\": acc_list_all, \n",
    "                                \"auc_list_all\": auc_list_all, \n",
    "                                \"f1_list_all\": f1_list_all,\n",
    "                                \"results_plot_all\": results_plot_all, \n",
    "                                \"results_data_all\": results_data_all, \n",
    "                                \"subset_dict_all\": subset_dict_all, \n",
    "                                }\n",
    "                \n",
    "                # pickle all the results to results summary folder\n",
    "                filename = f\"../results_summary/results_summary_{dataset}_{ns}.pickle\"\n",
    "\n",
    "                print(\"Saving file as pickle... \")\n",
    "                with open(filename, \"wb\") as f:\n",
    "                    pickle.dump(all_results, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                print(traceback.format_exc())\n",
    "                print(\"Error in computing performance metrics: \", e)\n",
    "                continue\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
